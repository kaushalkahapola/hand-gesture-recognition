{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1yphzN479LjAolCaKxll4BYlRXmBHgLuN",
      "authorship_tag": "ABX9TyP1MoOl1/jOV47uvKN4rvlA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushalkahapola/hand-gesture-recognition/blob/main/hand_gesture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get the dataset from Kaggle\n",
        "\n",
        "## About Dataset\n",
        "\n",
        "### Context\n",
        "\n",
        "Hand gesture recognition database is presented, composed by a set of near infrared images acquired by the Leap Motion sensor.\n",
        "\n",
        "### Content\n",
        "\n",
        "The database is composed by 10 different hand-gestures (showed above) that were performed by 10 different subjects (5 men and 5 women).\n",
        "\n",
        "The database is structured in different folders as:\n",
        "\n",
        "* /00 (subject with identifier 00)\n",
        "\n",
        "    * /01_palm (images for palm gesture of subject 00 )\n",
        "\n",
        "    * /01_palm/frame_197957_r.png,…,frame_198136_l.png, … (images that corresponds to different samples obtained for the palm gesture performed by the subject with identifier 00)\n",
        "\n",
        "    * /02_l (images for l gesture of subject 00 )\n",
        "    /10_down\n",
        "\n",
        "* /01\n",
        "\n",
        "* /02\n",
        "\n",
        "* /09 (last subject with identifier 09)\n",
        "\n",
        "Every root folder (00, 01,…) contains the infrared images of one subject. The folder name is the identifier of each different subject.\n",
        "\n",
        "https://www.kaggle.com/datasets/gti-upm/leapgestrecog/data"
      ],
      "metadata": {
        "id": "gL6tdyR5uSNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ezdmmstsAbO",
        "outputId": "80d4c66a-dcd1-4eb4-f950-28351bbc7628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_path = '/content/drive/MyDrive/hand_gesture_data'\n",
        "os.makedirs(data_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "IpvmEiRKs6sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmuEFORjtDVh",
        "outputId": "72ee74d5-a050-4d0d-b0a0-d33bb1613d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "EPG8y7ewtIP_",
        "outputId": "6fdf7075-6047-4d7e-8365-d9b13d7f9e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-33c8123c-52cc-4f5f-8a58-33855ffbadbd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-33c8123c-52cc-4f5f-8a58-33855ffbadbd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "8nsc9c03tklE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IIk-wNWcttng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFtsjf20t7Hn",
        "outputId": "0ea2e054-3505-4728-a608-a03feedd5f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                                  title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-------------------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "berkayalan/paris-2024-olympics-medals                                Paris 2024 Olympics Medals                           1KB  2024-08-14 11:02:45           4630         85  1.0              \n",
            "abdullahashfaqvirk/student-mental-health-survey                      Student Mental Health Survey                         2KB  2024-08-23 15:38:57           2078         36  0.88235295       \n",
            "haseebindata/student-performance-predictions                         Student Performance Predictions                      9KB  2024-08-17 06:57:57           4836        103  0.9411765        \n",
            "willianoliveiragibin/olympics-2024                                   Olympics 2024                                      352KB  2024-08-15 22:29:31           2143         34  1.0              \n",
            "balajivaraprasad/crimes-against-women-in-india-2001-2021             Crimes Against Women in India (2001-2021)           13KB  2024-08-15 16:47:18           2660         57  1.0              \n",
            "jeannicolasduval/2024-fortune-1000-companies                         2024 Fortune 1000 Companies                        301KB  2024-08-14 21:25:41           1211         25  1.0              \n",
            "uom190346a/ai-powered-job-market-insights                            AI-Powered Job Market Insights                      10KB  2024-08-26 05:55:43           1531         33  1.0              \n",
            "sahityasetu/paris-olympics-2024-total-medals-tally                   Paris Olympics 2024 Total Medals Tally               2KB  2024-08-23 20:22:40            711         22  0.88235295       \n",
            "myrios/cost-of-living-index-by-country-by-number-2024                Cost of Living Index by Country                      3KB  2024-07-19 06:25:42           8816        143  1.0              \n",
            "abdoomoh/daily-covid-19-data-2020-2024                               Daily COVID-19 Data (2020-2024)                      1MB  2024-08-26 19:04:56            695         22  1.0              \n",
            "souradippal/student-performance-prediction                           Student Performance Prediction                     381KB  2024-08-16 15:59:38           2290         51  1.0              \n",
            "syedafroz6284/fashion-nova-reviews                                   Fashion Nova Reviews                                 9MB  2024-08-24 12:54:10            636         23  0.8235294        \n",
            "ahsenwaheed/youtube-comments-spam-dataset                            Youtube Comments Spam Dataset                      159KB  2024-08-12 16:31:41            577         21  1.0              \n",
            "emreksz/software-engineer-jobs-and-salaries-2024                     Software Engineer Jobs & Salaries 2024              23KB  2024-08-12 00:08:03           2515         53  1.0              \n",
            "spypsc07/amazon-products                                             Amazon_products                                      6KB  2024-08-02 06:13:47           2415         33  1.0              \n",
            "shashanks1202/apartment-rent-data                                    Apartment Rent Data                                 27MB  2024-08-16 11:11:12           1305         35  0.8235294        \n",
            "crinklybrain2003/pokmon-base-stats-dataset                           Pokémon Base Stats Dataset                          37KB  2024-08-26 02:04:25            839         31  1.0              \n",
            "mahmoudemadabdallah/hr-analytics-employee-attrition-and-performance  HR Analytics Employee Attrition & Performance      147KB  2024-08-21 15:33:00           1369         37  1.0              \n",
            "sudhanvahg/gdp-and-productivity-of-indian-cities-2019-2024           GDP and Productivity of Indian Cities (2019-2024)    5KB  2024-08-16 08:13:43            586         22  1.0              \n",
            "mrmars1010/filpkart-mobiles                                          Flipkart Products Sales: Mobiles Dataset 📱         108KB  2024-08-24 04:31:50           1341         29  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d gti-upm/leapgestrecog -p /content/drive/MyDrive/hand_gesture_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF_FIYXWt_K5",
        "outputId": "6cec4167-20a5-4e22-e5b6-3b4cb7d73d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/gti-upm/leapgestrecog\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading leapgestrecog.zip to /content/drive/MyDrive/hand_gesture_data\n",
            "100% 2.13G/2.13G [00:26<00:00, 106MB/s] \n",
            "100% 2.13G/2.13G [00:26<00:00, 85.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/hand_gesture_data/leapgestrecog.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/hand_gesture_data')\n"
      ],
      "metadata": {
        "id": "XzHH4LgruEOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# for root, dirs, files in os.walk('/content/drive/MyDrive/hand_gesture_data/leapgestrecog'):\n",
        "#     print(root, dirs, files)\n"
      ],
      "metadata": {
        "id": "0j2fp9LAuPPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preparing the data"
      ],
      "metadata": {
        "id": "PF9VFcrayKbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths_and_labels(base_dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through each subject's folder\n",
        "    for subject in os.listdir(base_dir):\n",
        "        subject_path = os.path.join(base_dir, subject)\n",
        "\n",
        "        # Loop through each gesture folder\n",
        "        for gesture in os.listdir(subject_path):\n",
        "            gesture_path = os.path.join(subject_path, gesture)\n",
        "\n",
        "            # Loop through each image file\n",
        "            for img_file in os.listdir(gesture_path):\n",
        "                image_paths.append(os.path.join(gesture_path, img_file))\n",
        "                labels.append(gesture)  # Assuming gesture folder names are the labels\n",
        "\n",
        "    return image_paths, labels"
      ],
      "metadata": {
        "id": "f2gKA8AIwh1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/hand_gesture_data/leapGestRecog'\n",
        "image_paths, labels = get_image_paths_and_labels(data_dir)"
      ],
      "metadata": {
        "id": "uvAxTpo_xbf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of images:\", len(image_paths))\n",
        "print(\"Number of labels:\", len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnvYXa-4xrEa",
        "outputId": "a8f689e2-a223-4114-8370-46e85f6c93c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 20000\n",
            "Number of labels: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):  # Display the first 5 samples\n",
        "    print(f\"Image Path: {image_paths[i]}\")\n",
        "    print(f\"Label: {labels[i]}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSORiZqIxtsw",
        "outputId": "1e8f77f9-c485-4c3c-ba79-d9661050216c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Path: /content/drive/MyDrive/hand_gesture_data/leapGestRecog/00/01_palm/frame_00_01_0001.png\n",
            "Label: 01_palm\n",
            "---\n",
            "Image Path: /content/drive/MyDrive/hand_gesture_data/leapGestRecog/00/01_palm/frame_00_01_0002.png\n",
            "Label: 01_palm\n",
            "---\n",
            "Image Path: /content/drive/MyDrive/hand_gesture_data/leapGestRecog/00/01_palm/frame_00_01_0003.png\n",
            "Label: 01_palm\n",
            "---\n",
            "Image Path: /content/drive/MyDrive/hand_gesture_data/leapGestRecog/00/01_palm/frame_00_01_0004.png\n",
            "Label: 01_palm\n",
            "---\n",
            "Image Path: /content/drive/MyDrive/hand_gesture_data/leapGestRecog/00/01_palm/frame_00_01_0005.png\n",
            "Label: 01_palm\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for path in image_paths[:5]:  # Check the first 5 paths\n",
        "    if not os.path.isfile(path):\n",
        "        print(f\"File not found: {path}\")\n"
      ],
      "metadata": {
        "id": "LcQ8oJBExwmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = set(labels)\n",
        "print(\"Unique Labels:\", unique_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4BjMfsbyEht",
        "outputId": "cfcd07f7-573d-44ab-9fa6-34ba613bea50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels: {'01_palm', '02_l', '03_fist', '04_fist_moved', '09_c', '08_palm_moved', '10_down', '06_index', '05_thumb', '07_ok'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def show_images(image_paths, labels, num_images=5):\n",
        "    for i in range(num_images):\n",
        "        img = cv2.imread(image_paths[i], cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (224, 224))  # Resize to the size expected by the model\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(labels[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_images(image_paths, labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "KQLAi1wLyT3r",
        "outputId": "21b18e44-0b95-40e4-f419-a1364b5488a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define data augmentation for training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define data generator without augmentation for validation data\n",
        "val_datagen = ImageDataGenerator()\n"
      ],
      "metadata": {
        "id": "RSYxHU_0yp9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_labels = np.array(labels)\n",
        "y_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAz5xBpQ0QXD",
        "outputId": "ef067575-ee21-4400-8810-7a6e7ccd58d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['01_palm', '01_palm', '01_palm', ..., '10_down', '10_down',\n",
              "       '10_down'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = np.unique(y_labels)\n",
        "unique_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOaKbQWp4X2f",
        "outputId": "6c0e34be-94ca-4b03-e687-25eaa21cbe13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['01_palm', '02_l', '03_fist', '04_fist_moved', '05_thumb',\n",
              "       '06_index', '07_ok', '08_palm_moved', '09_c', '10_down'],\n",
              "      dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boolean_y = [label == unique_classes for label in y_labels]\n",
        "boolean_y[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8oTBRsP4uby",
        "outputId": "4144f7db-5fff-4483-a71f-9caa2126535e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ True, False, False, False, False, False, False, False, False,\n",
              "        False]),\n",
              " array([ True, False, False, False, False, False, False, False, False,\n",
              "        False])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(image_paths)\n",
        "y = np.array(boolean_y)\n",
        "\n",
        "# Set number of images to use for experimenting\n",
        "NUM_IMAGES = 1000\n",
        "\n",
        "# split our data into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split them into training and validation of total size NUM_IMAGES\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],\n",
        "                                                  y[:NUM_IMAGES],\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42)\n",
        "\n",
        "len(X_train), len(y_train), len(X_val), len(y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCLzpaX_4_7k",
        "outputId": "0a8290d7-fdc6-435d-be70-0d626c0ff767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 800, 200, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:1], y_train[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_nkKBbt5VK0",
        "outputId": "4727a306-e6c4-488b-8ec7-7fcde129257c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['/content/drive/MyDrive/hand_gesture_data/leapGestRecog/00/01_palm/frame_00_01_0030.png'],\n",
              "       dtype='<U92'),\n",
              " array([[ True, False, False, False, False, False, False, False, False,\n",
              "         False]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "EzMi-_wX5dAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Size define\n",
        "IMG_SIZE = 224\n",
        "\n",
        "def process_image(path,img_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Takes an image file path and turns into a tensor\n",
        "  \"\"\"\n",
        "\n",
        "  #read in an image file\n",
        "  image = tf.io.read_file(path)\n",
        "\n",
        "  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n",
        "  image = tf.image.decode_jpeg(image, channels=1)\n",
        "\n",
        "  # Convert the colour channel values from 0-255 to 0-1 values\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  # Resize the image to our desired value (224, 224)\n",
        "  image = tf.image.resize(image, size=(img_size,img_size))\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "pLYTN5Nx5ZC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_label(image_path, label):\n",
        "  \"\"\"\n",
        "  Takes an image file path name and the assosciated label,\n",
        "  processes the image and reutrns a typle of (image, label).\n",
        "  \"\"\"\n",
        "  return process_image(image_path), label"
      ],
      "metadata": {
        "id": "vkOnlFkd5hAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size, 32 is a good start\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create a function to turn data into batches\n",
        "def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n",
        "  \"\"\"\n",
        "  Creates batches of data out of image (X) and label (y) pairs.\n",
        "  Shuffles the data if it's training data but doesn't shuffle if it's validation data.\n",
        "  Also accepts test data as input (no labels).\n",
        "  \"\"\"\n",
        "  # If the data is a test dataset, we probably don't have have labels\n",
        "  if test_data:\n",
        "    print(\"Creating test data batches...\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths (no labels)\n",
        "    data_batch = data.map(process_image).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "\n",
        "  # If the data is a valid dataset, we don't need to shuffle it\n",
        "  elif valid_data:\n",
        "    print(\"Creating validation data batches...\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths\n",
        "                                               tf.constant(y))) # labels\n",
        "    data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "\n",
        "  else:\n",
        "    print(\"Creating training data batches...\")\n",
        "    # Turn filepaths and labels into Tensors\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                               tf.constant(y)))\n",
        "    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images\n",
        "    data = data.shuffle(buffer_size=len(X))\n",
        "\n",
        "    # Create (image, label) tuples (this also turns the iamge path into a preprocessed image)\n",
        "    data = data.map(get_image_label)\n",
        "\n",
        "    # Turn the training data into batches\n",
        "    data_batch = data.batch(BATCH_SIZE)\n",
        "  return data_batch"
      ],
      "metadata": {
        "id": "VYwflrtq5nRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation data batches\n",
        "train_data = create_data_batches(X_train, y_train)\n",
        "val_data = create_data_batches(X_val, y_val, valid_data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tb1e-x35yQg",
        "outputId": "2b3d7907-c7d8-408a-c9a4-1c96a757ace4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training data batches...\n",
            "Creating validation data batches...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.element_spec, val_data.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4odTck-51pB",
        "outputId": "c64074ae-8512-4fc8-9c5d-345636ef89d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, 10), dtype=tf.bool, name=None)),\n",
              " (TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, 10), dtype=tf.bool, name=None)))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Traning Our Model with a subset of our data"
      ],
      "metadata": {
        "id": "IzfLgfS0iZ0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup input shape to the model\n",
        "INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 1] # batch, height, width, colour channels\n",
        "\n",
        "# Setup output shape of our model\n",
        "OUTPUT_SHAPE = len(unique_classes)"
      ],
      "metadata": {
        "id": "eyj5CIoU59V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tf_keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "LX_MSzoQ6Lc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function which builds a Keras model\n",
        "def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE):\n",
        "\n",
        "  # Define your model\n",
        "  model = Sequential([\n",
        "      Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
        "      MaxPooling2D((2, 2)),\n",
        "      Flatten(),\n",
        "      Dense(128, activation='relu'),\n",
        "      Dense(units=output_shape, activation='softmax')  # Number of classes\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  # Build the model\n",
        "  model.build()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "x8RiBM9n6Gye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "qZ_i-nMA6SUV",
        "outputId": "9a5c209f-b99d-4497-da13-6a71b164a41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m394272\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │      \u001b[38;5;34m50,466,944\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">394272</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">50,466,944</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,468,554\u001b[0m (192.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,468,554</span> (192.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,468,554\u001b[0m (192.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,468,554</span> (192.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\n",
        "                                                  patience=3)"
      ],
      "metadata": {
        "id": "m5J2hmSp9D73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 100\n",
        "\n",
        "# Build a function to train and return a trained model\n",
        "def train_model():\n",
        "  \"\"\"\n",
        "  Trains a given model and returns the trained version.\n",
        "  \"\"\"\n",
        "  # Create a model\n",
        "  model = create_model()\n",
        "\n",
        "\n",
        "  # Fit the model to the data passing it the callbacks we created\n",
        "  model.fit(x=train_data,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_data=val_data,\n",
        "            validation_freq=1,\n",
        "            callbacks=[early_stopping])\n",
        "  # Return the fitted model\n",
        "  return model"
      ],
      "metadata": {
        "id": "PfB0g_P8__Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data\n",
        "model = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQjVgZEoAOxU",
        "outputId": "7b020e8e-f24c-4ea7-f7fc-06c47aa3a8e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 15s/step - accuracy: 0.4985 - loss: 4.4088 - val_accuracy: 0.9900 - val_loss: 0.0460\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 175ms/step - accuracy: 0.9861 - loss: 0.0530 - val_accuracy: 1.0000 - val_loss: 0.0112\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 0.0058\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.0021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training the model with full data"
      ],
      "metadata": {
        "id": "4F0ci4ybimu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = create_data_batches(X, y)\n",
        "full_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0-DpTflARcO",
        "outputId": "dca8dc41-9203-4c3c-e246-6c3bd6295871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training data batches...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.bool, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model for full model\n",
        "full_model = create_model()"
      ],
      "metadata": {
        "id": "Zpukct2HDR95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\",\n",
        "                                                             patience=3)"
      ],
      "metadata": {
        "id": "fnJQRn7XDYX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the full model to the full data\n",
        "full_model.fit(x=full_data,\n",
        "               epochs=NUM_EPOCHS,\n",
        "               callbacks=[full_model_early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTlcAUbkDfFa",
        "outputId": "ee104ad2-d3c4-4b23-dfd8-6b24a62b4824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7478s\u001b[0m 12s/step - accuracy: 0.8149 - loss: 0.7681\n",
            "Epoch 2/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0025\n",
            "Epoch 3/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 4.3601e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 1.8637e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 9.7605e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 6.2021e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b4a1d892800>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Saving the model"
      ],
      "metadata": {
        "id": "rpSbFxBiitru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Create a function to save a model\n",
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  Saves a given model in a models directory and appends a suffix (string).\n",
        "  \"\"\"\n",
        "  # Create a model directory pathname with current time\n",
        "  modeldir = os.path.join(\"/content/drive/MyDrive/hand_gesture_data/models\",\n",
        "                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n",
        "  model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n",
        "  print(f\"Saving model to: {model_path}...\")\n",
        "  model.save(model_path)\n",
        "  return model_path"
      ],
      "metadata": {
        "id": "argvsL8_DhYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(full_model, suffix=\"full-image-set-model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "I2klxZ_jjAGJ",
        "outputId": "c9e4f8f3-9970-4ea9-9f7b-9438a9ef8253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: /content/drive/MyDrive/hand_gesture_data/models/20240831-17471725126453-full-image-set-model.h5...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/hand_gesture_data/models/20240831-17471725126453-full-image-set-model.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/hand_gesture_data/models/full-set-new.keras')"
      ],
      "metadata": {
        "id": "vltM8WLhjKIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0OCnBcyvjfXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}